{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZZS6o0Z8Oj"
      },
      "source": [
        "# [실습3] IMDB 영화 후기 데이터셋 벡터화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfwyzgmXZ8Ou"
      },
      "source": [
        "IMDB 데이터셋을 직접 다운로드하여 벡터화하는 과정을 살펴본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaO7ilwSZ8Ou"
      },
      "source": [
        "준비 과정 1: 데이터셋 다운로드 압축 풀기\n",
        "\n",
        "압축을 풀면 아래 구조의 디렉토리가 생성된다.\n",
        "\n",
        "```\n",
        "aclImdb/\n",
        "...train/\n",
        "......pos/\n",
        "......neg/\n",
        "...test/\n",
        "......pos/\n",
        "......neg/\n",
        "```\n",
        "\n",
        "`train`의 `pos`와 `neg` 서브디렉토리에 각각 12,500개의 긍정과 부정 후기가\n",
        "포함되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScCXFlWZ8Ou",
        "outputId": "e64fb37f-d5a6-49ac-f8b9-0567bb3ce0b2"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMRKwRM9Z8Ou"
      },
      "source": [
        "`aclImdb/train/unsup` 서브디렉토리는 필요 없기에 삭제한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xsKw4vPZ8Ou"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "if platform.system() == 'Linux':\n",
        "    !rm -r aclImdb/train/unsup\n",
        "else:\n",
        "    import shutil\n",
        "    unsup_path = './aclImdb/train/unsup'\n",
        "    shutil.rmtree(unsup_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rZ3a9ETZ8Ou"
      },
      "source": [
        "긍정 후기 하나의 내용을 살펴보자.\n",
        "모델 구성 이전에 훈련 데이터셋을 살펴 보고\n",
        "모델에 대한 직관을 갖는 과정이 항상 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK5FRFX0Z8Ou",
        "outputId": "1304a433-6e6f-4ad0-f0e7-80b49147acb4"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !cat aclImdb/train/pos/4077_10.txt\n",
        "else:\n",
        "    with open('aclImdb/train/pos/4077_10.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeoV0YCZ8Ov"
      },
      "source": [
        "준비 과정 2: 검증셋 준비\n",
        "\n",
        "훈련셋의 20%를 검증셋으로 떼어낸다.\n",
        "이를 위해 `aclImdb/val` 디렉토리를 생성한 후에\n",
        "긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX30tWVBZ8Ov"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)            # val 디렉토리 생성\n",
        "    files = os.listdir(train_dir / category)\n",
        "\n",
        "    random.Random(1337).shuffle(files)         # 훈련셋 무작위 섞기\n",
        "\n",
        "    num_val_samples = int(0.2 * len(files))    # 20% 지정 후 검증셋으로 옮기기\n",
        "    val_files = files[-num_val_samples:]\n",
        "\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBC3puZ8Ov"
      },
      "source": [
        "준비 과정 3: 텐서 데이터셋 준비\n",
        "\n",
        "`text_dataset_from_directory()` 함수를 이용하여\n",
        "훈련셋, 검증셋, 테스트셋을 준비한다.\n",
        "자료형은 모두 `Dataset`이며, 배치 크기는 32를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41g6BtPZ8Ov",
        "outputId": "8fa59267-238d-44c7-87f4-33df0e3e15b1"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLi_rBmlZ8Ov"
      },
      "source": [
        "각 데이터셋은 배치로 구분되며\n",
        "입력은 `tf.string` 텐서이고, 타깃은 `int32` 텐서이다.\n",
        "크기는 모두 32이며 지정된 배치 크기이다.\n",
        "예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G05FSfO1Z8Ov",
        "outputId": "8ca36337-3ce3-4a51-d35b-ce16eafbc25b"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "\n",
        "    # 예제: 첫째 배치의 첫째 후기\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3uJTQVlJSy"
      },
      "source": [
        "### 11.3.3 시퀀스 활용법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZ34Qg7lJSy"
      },
      "source": [
        "**정수 벡터 데이터셋 준비**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoVifu3DlJSy"
      },
      "source": [
        "훈련셋의 모든 후기 문장을 정수들의 벡터로 변환한다.\n",
        "단, 후기 문장이 최대 600개의 단어만 포함하도록 한다.\n",
        "또한 사용되는 어휘는 빈도 기준 최대 2만 개로 제한한다.\n",
        "\n",
        "- `max_length = 600`\n",
        "- `max_tokens = 20000`\n",
        "- `output_sequence_length=max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orh58qTYlJSy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "# 어휘 색인 생성 대상 훈련셋 후기 텍스트 데이터셋\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94MVnSHlJSy"
      },
      "source": [
        "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
        "`output_sequence_length=600`으로 지정하였기에 모든 문장은 단어를 최대 600개에서\n",
        "잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.\n",
        "물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다.\n",
        "또한 벡터에 사용된 정수는 2만보다 작은 값이며,\n",
        "이는 빈도가 가장 높은 2만개의 단어만을 대상(`max_tokens=20000`)으로 했기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B-SbA7rlJSy",
        "outputId": "a951c537-5db8-4bfe-eda4-346b2e4145b7"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in int_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrjK1-zUwFj"
      },
      "source": [
        "**트랜스포머 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH2T-AQOUwFj"
      },
      "source": [
        "위 그림에서 설명된 트랜스포머 인코더를 층(layer)으로 구현하면 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6g33CzrUwFk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtXomfGDUwFk"
      },
      "source": [
        "**트랜스포머 인코더 활용 모델**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPSowlcGUwFk"
      },
      "source": [
        "훈련 데이터셋이 입력되면 먼저 단어 임베딩을 이용하여\n",
        "단어들 사이의 연관성을 찾는다.\n",
        "이후 트랜스포머 인코더로 셀프 어텐션을 적용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxbIVhi3UwFk",
        "outputId": "77b51533-c428-4b80-b704-24227fc076cf"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "# 길이가 600인 1차원 어레이로 변환\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rJ82rMIUwFl"
      },
      "source": [
        "훈련 과정은 특별한 게 없다.\n",
        "테스트셋에 대한 정확도가 87.5% 정도로 바이그램 모델보다 좀 더 낮다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2BDEYHaUwFl",
        "outputId": "a81a5913-fdf8-44f9-f2ce-d4f983479c51"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tEOxNV_UwFl"
      },
      "source": [
        "**단어 위치 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHQszXy-UwFl"
      },
      "source": [
        "다음 `PositionalEmbedding` 층 클래스는 두 개의 임베딩 클래스를 사용한다.\n",
        "하나는 보통의 단어 임베딩이며,\n",
        "다른 하나는 단어의 위치 정보를 임베딩한다.\n",
        "각 임베딩의 출력값을 합친 값을 트랜스포머에게 전달하는 역할을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRiJ-vBXUwFm"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG2Db4SRUwFm"
      },
      "source": [
        "**단어위치인식 트랜스포머 아키텍처**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq24k0wlUwFm"
      },
      "source": [
        "아래 코드는 `PositionalEmbedding` 층을 활용하여 트랜스포머 인코더가\n",
        "단어위치를 활용할 수 있도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYcxHlk7UwFm"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTqdNRiliiYQ",
        "outputId": "ba70a092-839a-4307-ec8a-a28610059829"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt5VbzzNiiYQ",
        "outputId": "ba70a092-839a-4307-ec8a-a28610059829"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c86b3592b6800d985c04531f2c445f0fa6967131b8dd6395a925f7622e55602"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
